{"cells":[{"metadata":{},"cell_type":"markdown","source":"**University of Edinburgh**\\\n**School of Mathematics**\\\n**Bayesian Data Analysis, 2020/2021, Semester 2**\\\n**Daniel Paulin & Nicol√≤ Margaritella**\n\n**Solutions for Workshop 4: Bayesian Generalised\nLinear Models (GLMs) for count and binary data**\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"#This code loads a compiled version of JAGS and rjags from a zip file on Onedrive, and loads rjags. It should only take a few seconds.\n#IMPORTANT: Go to the Kaggle Settings (right hand side click on K icon) and enable the Internet option in Settings before running this.\nsystem(\"wget --no-check-certificate -r 'https://uoe-my.sharepoint.com/:u:/g/personal/dpaulin_ed_ac_uk/EX_-yUc-bIZJhLXHcZxpOj8Ba6dwC15X_MjYoox-xM2KlQ?download=1' -O /kaggle/working/kaggle_JAGS.zip\")\nsystem(\"unzip /kaggle/working/kaggle_JAGS.zip\")\nsystem(\"rm /kaggle/working/kaggle_JAGS.zip\")\nsystem(\"cd /kaggle/working/JAGS-4.3.0\")\nsystem(\"make install\")\nlibrary(rjags,lib.loc=\"/kaggle/working\")\n#If it ran correctly, you should see \n#Loading required package: coda\n#Linked to JAGS 4.3.0\n#Loaded modules: basemod,bugs\n\n#In case you are still experiencing difficulties with this, please use the following code (this compiles and installs JAGS from the source, it takes 6-7 minutes):\n#system(\"wget https://sourceforge.net/projects/mcmc-jags/files/JAGS/4.x/Source/JAGS-4.3.0.tar.gz -P /kaggle/working\")\n#system(\"tar xvfz /kaggle/working/JAGS-4.3.0.tar.gz\")\n#system(\"cd /kaggle/working/JAGS-4.3.0\")\n#system(\"/kaggle/working/JAGS-4.3.0/configure\")\n#system(\"make\")\n#system(\"make install\")\n#install.packages(\"rjags\", lib=\"/kaggle/working\")\n#library(rjags,lib.loc=\"/kaggle/working\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 1.  **Modelling fatal airline accidents from 1976 through 2001.**\n\n   **This exercise has been taken largely from a shortcourse at the University of Copenhagen which occurred in\nJanuary 2013 and notes from Gurrin, Carstensen, Hojsgaard, and Ekstrom. The dataset `airline.RData` is available on\n    Learn but it will be automatically uploaded by the code below.**\n\n   **The fields are:**  \n- **Year1975 (number of years after 1975),**\n- **Year,**\n- **Fatal (number of fatal airline accidents),**\n- **Miles (total passenger miles,in $10^{11}$ miles, e.g., $3.863 = 3.683*10^{11} \\text{miles} = 368.3$ Billion miles),**\n- **Rate (fatalities per $10^{11}$ passenger miles).**\n    \n   **You will be fitting 3 separate Poisson models to Fatal.**\n    \n\n**(i) Conduct some exploratory data analysis:**\n- **Plot fatalities against year. Which year had the most fatalities?**\n- **Plot miles flown against year. What do you see?**\n- **Now plot the rate against year. What do you think about how dangerous flying is?**\n\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"system(\"wget --no-check-certificate -r 'https://drive.google.com/uc?export=download&id=19pnVieqVqqxnPLjvb-pYSEVCWoYs5rHr' -O /kaggle/working/airline.RData\")\n# You need to enable the Internet in Settings in Kaggle (right hand side menu) before running this\n#airlines<-read.csv(\"/kaggle/working/airline.csv\",header=TRUE,sep=\"\\t\")\nload(\"/kaggle/working/airline.RData\")\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"par(mfrow=c(2,2))\nhist(airlines$fatal,main=\"Annual airline fatalities: 1976-2001\")\nplot(airlines$year,airlines$fatal,type=\"b\",main=\"Fatalities by Year\")\nplot(airlines$year,airlines$miles,type=\"b\",main=\"Miles (10^11) by Year\")\nplot(airlines$year,airlines$rate,type=\"b\",\nmain=\"Fatalities/Mile (10^11) by Year\")\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**<u>Constant Expected Fatality Model</u>. Assume that the number of fatalities each year\ncomes from a single Poisson distribution with unknown mean parameter.**\n\n**(ii a) Carry out a frequentist analysis using the `glm` function, the Poisson family and\nthe default log link function (Hint: the formula $y\\sim1$ fits a model with constant\nmean). Report the mle in the original, non-transformed scale.**\n\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"# fits a constant mu\nm1.log <- glm(fatal ~ 1,family=poisson(link=\"log\"),data=airlines)\nround(exp(m1.log$coefficients),5)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**(ii b) Fit the model with the sqrt link function. Square the estimated coeffcient to see\nhow much the mle changed.**"},{"metadata":{"trusted":true},"cell_type":"code","source":"m1.sqrt <- glm(fatal ~ 1,family=poisson(link=\"sqrt\"),data=airlines)\nround((m1.sqrt$coefficients)^2,5)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**(ii c) Use JAGS to carry out a Bayesian analysis of this constant mortality model using a\nGamma (a,b) prior for $\\mu$ with parameters a=1 and b=0.02.**\n\n**Use 3 initial values for $\\mu$ of 10, 50, and 100. Check for convergence and mixing in 5 ways:\ntrace plots, BGR statistic, autocorrelation plots, effective sample size calculations, and\nseeing if the Monte Carlo error < 1/20th the standard deviation.**\n\n**What is the posterior mean for $\\mu$? Obtain the 95\\% symmetric Credible Interval for $\\mu$**"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Create data input for JAGS\nairlines.data <- list(n=nrow(airlines),fatal=airlines$fatal)\n# Create initial values for JAGS\nnum.chains <- 3\nairlines.inits <- list(list(mu=10),list(mu=50),list(mu=100))\n# Create model block for JAGS\nairlines.model <- \"model {\n# data that will be read in are n and fatal\n#Hyperparameters\ngamma.a <- 1\ngamma.b <- 0.02\n# prior\nmu ~ dgamma(gamma.a,gamma.b)\n#Likelihood\nfor(i in 1:n){fatal[i] ~ dpois(mu) }\n}\"\n# Run JAGS\nburnin <- 2000\ninference.length <- 10000\nresults.const.A <- jags.model(file=textConnection(airlines.model),\ndata=airlines.data, inits=airlines.inits,\nn.chains=num.chains, quiet = TRUE)\n#\nupdate(results.const.A, n.iter=burnin)\n#\nresults.const.B <- coda.samples(results.const.A, variable.names=c(\"mu\"),\nn.iter=inference.length)\n#\n# Check for convergence before looking at posterior dist and summaries\nplot(results.const.B)\ngelman.plot(results.const.B)\ngelman.diag(results.const.B)\nautocorr.plot(results.const.B[[1]][,\"mu\"],main=\"Poisson mu\")\neffectiveSize(results.const.B[[1]][,\"mu\"])\nsummary(results.const.B) \n# posterior mean = 24.4\n# 95% CI [22.55, 26.34]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**(ii d) Modify your JAGS code to predict the 2002 fatal accidents and produce a 95\\% predictive\ninterval. To do this, modify the input data by increasing n to n+1 and append an NA\nvalue to the vector of fatal values, and then request that `fatal[27]` be output in the\n`coda` call:**"},{"metadata":{"trusted":true},"cell_type":"markdown","source":"`airlines.data <- list(n=nrow(airlines)+1,fatal=c(airlines$fatal,NA))\nlength(airlines.data$fatal\n...\nresults.const.B <- coda.samples(results.const.A,\nvariable.names=c(\"mu\",\"fatal[27]\"),n.iter=10000)`"},{"metadata":{"trusted":true},"cell_type":"code","source":"airlines.data <- list(n=nrow(airlines)+1,fatal=c(airlines$fatal,NA))\n#\nresults.const.A <- jags.model(file=textConnection(airlines.model),\ndata=airlines.data, inits=airlines.inits,n.chains=num.chains, quiet = TRUE)\n#\nupdate(results.const.A, n.iter=burnin)\n#\nresults.const.B <- coda.samples(results.const.A,\nvariable.names=c(\"mu\",\"fatal[27]\"),n.iter=10000)\n# Usual way of summarising\nsummary(results.const.B) # The interval is [15, 35]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**<u>Constant Fatality Rate, per mile, Model</u>. Use JAGS to fit a Poisson model where $\\mu$\nis proportional to the number of passenger miles, namely, $\\mu_{i} = \\lambda*\\text{miles}_{i}$.**\n\n**(iii a) In the JAGS code, mu[i] <- lambda * miles[i] and fatal[i]~dpois(mu[i]). Thus $\\lambda$ is the new\nparameter with its own prior, hyperparameters, and initial values, and you need to add miles\nto the data block. For hyperparameters, multiply those calculated in (ii c) by 0.1. Use initial\nvalues for $\\lambda$ of 1, 5, and 50. As before, check for convergence and mixing.**\n\n**Predict the number of fatal accidents for 2002 assuming there were 20 passenger miles ($10^{11}$)\nflown. Look at the trace plots and BGR statistic. What is the posterior mean for $\\lambda$? The\n95% predictive interval for number of accidents in 2002?**"},{"metadata":{"trusted":true},"cell_type":"code","source":"# --- (iii) Constant Fatality Rate (per mile)\n# --- Model is now Poisson(mu[i] = lambda*miles[i])\nairlines.miles.data <- list(n=nrow(airlines)+1,fatal=c(airlines$fatal,NA),\nmiles=c(airlines$miles,20))\n# Create initial values for JAGS\nnum.chains <- 3\nairlines.miles.inits <- list(list(lambda=1),list(lambda=5), list(lambda=50))\n# Create model block for JAGS\nairlines.miles.model <- \"model {\n# data that will be read in are n, fatal, and miles\n#Hyperparameters\ngamma.a <- 0.1\ngamma.b <- 0.002\n# prior\nlambda ~ dgamma(gamma.a,gamma.b)\n#Likelihood\nfor(i in 1:n){\nmu[i] <- lambda*miles[i]\nfatal[i] ~ dpois(mu[i]) }\n}\"\nresults.miles.A <- jags.model(file=textConnection(airlines.miles.model),\ndata=airlines.miles.data,\ninits=airlines.miles.inits,\nn.chains=num.chains, quiet = TRUE)\n#\nupdate(results.miles.A, n.iter=burnin)\n#\nresults.miles.B <- coda.samples(results.miles.A,\nvariable.names=c(\"lambda\",\"fatal[27]\"),\nn.iter=inference.length)\n# (Convergence checks)\nplot(results.miles.B)\ngelman.plot(results.miles.B)\ngelman.diag(results.miles.B)\nautocorr.plot(results.miles.B[[1]][,\"lambda\"],main=\"Poisson rate lambda\")\neffectiveSize(results.miles.B[[1]][,\"lambda\"])\nsummary(results.miles.B)\n#Posterior Mean Lambda = 2.30\n#Predictive interval fatal[27]= [33,60]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**<u>Rate as a Function of Time Model</u>. What if you modeled the mean parameter $\\mu$ as a linear function of time, i.e., for year t:$\\mu(t) = \\beta_0 + \\beta_1 t$. $\\beta_1$ is presumably a negative number as fatal accidents are decreasing with\ntime. What could be a problem?**\n\n**To avoid this potential problem but allow for a time effect on $\\mu$, we will now model the rate\nparameter $\\lambda$, as an exponentiated linear function of (centred) time:\n$$\\lambda(t)=\\exp{(\\beta_0 + \\beta_1 (t-\\bar{t}))}$$**\n\n**The resulting Poisson parameter for year t:\n$$\\mu(t)=\\lambda(t)*\\text{miles}(t)=\\exp{(\\beta_0 + \\beta_1 (t-\\bar{t}))}*\\text{miles}(t)$$**\n\n**With a log link function for $\\mu(t)$, the resulting transformation:\n$$log(\\mu(t))=\\beta_0 + \\beta_1 (t-\\bar{t})+log(\\text{miles}(t))$$**\n\n**which is not \"entirely\" a linear function of $t$ due to the $log(\\text{miles}(t))$ term. However, the log\ntransformed rate parameter is linear in time:$ ln(\\lambda(t)) = \\beta_0+\\beta_1(t-\\bar{t})$. When the link function\nof the expected value is the sum of a linear combination of covariates and a known constant,\nin this case $log(\\text{miles}(t))$, that constant is called an *offset*.**\n\n**(iv a) Calculate crude estimates of $\\beta_0$ and $\\beta_1$ by using `lm` to regress $log(fatal(t))$ on year $t$\n(Year1975) with an offset. Here's one way to do this in R:**\n\n\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"airlines$ctrd.time <- airlines$year1975-mean(airlines$year1975)\nm2 <- lm(log(fatal) ~ ctrd.time + offset(log(miles)),data=airlines)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**What is the estimated year effect (on the log scale)? How does the expected rate change\nfrom year t to t + 1 (what is the multiplicative effect)?**"},{"metadata":{"trusted":true},"cell_type":"code","source":"airlines$ctrd.time <- airlines$year1975-mean(airlines$year1975)\nm2 <- lm(log(fatal) ~ ctrd.time + offset(log(miles)),data=airlines)\nexp(coef(m2)[2]) # The multiplicative factor between t and t+1\n## ctrd.time\n## 0.9325075\n# ~7% decrease x year","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**(iv b) Use the `glm` function to calculate the MLEs for $\\beta_0$ and $\\beta_1$ in this model. What is the estimated year effect now? And how does the expected rate change each year?**"},{"metadata":{"trusted":true},"cell_type":"code","source":"m2.glm <- glm(fatal ~ ctrd.time + offset(log(miles)), data=airlines,\nfamily = poisson)\ncoef(m2.glm)\n## (Intercept) ctrd.time\n## 0.93551488 -0.06874189\nexp(coef(m2.glm)[2])\n## ctrd.time\n## 0.9335676\n#m2.glm$deviance\n## [1] 22.54528","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**(iv c) Carry out a Bayesian analysis of this model. You will need to specify priors for $\\beta_0$ and\n$\\beta_1$. For simplicity, use wide Normal priors with mean 0 and variance 1000. Use 3 sets of\ninitial values for $\\beta_0$ and $\\beta_1$ randomly drawn from normal distributions with variance 100. Check for convergence and mixing as before.**\n\n**Again predict 2002 fatal accidents using miles = 20, and plug in time=27. How does\nthe posterior mean for $\\beta_1$ compare to the MLE? How does the 95% prediction interval\nfor 2002 differ from the previous results?**"},{"metadata":{"trusted":true},"cell_type":"code","source":"# ---- Bayesian model (iv c)\n# Model is now Poisson(mu[i] = exp(b0+b1*time[i])miles[i])\nairlines.time.data <- list(n=nrow(airlines)+1,fatal=c(airlines$fatal,NA),\ntime=c(airlines$year1975,27),\nmiles=c(airlines$miles,20))\n# Create initial values for JAGS\nnum.chains <- 3\nairlines.time.inits <- function(){ \nbeta0 <- rnorm(1,0,10)\nbeta1 <- rnorm(1,0,10)\nreturn( list(beta0=beta0, beta1=beta1) )\n}\n# Create model block for JAGS\nairlines.time.model <- \"model {\n# data that will be read in are n, fatal, time and miles\n# prior\nbeta0 ~ dnorm(0,0.001)\nbeta1 ~ dnorm(0,0.001)\n#Likelihood\nfor(i in 1:n) {\nlog(mu[i]) <- beta0+beta1*(time[i]-mean(time[])) + log(miles[i])\nfatal[i] ~ dpois(mu[i]) }\n}\"\n# Run JAGS to the completion of the \"adaption\" stage\nresults.time.A <- jags.model(file=textConnection(airlines.time.model),\ndata=airlines.time.data,\ninits=airlines.time.inits,\nn.chains=num.chains, quiet = TRUE)\n#\nupdate(results.time.A, n.iter=burnin)\n#\nresults.time.B <- coda.samples(results.time.A,\nvariable.names=c(\"beta0\",\"beta1\",\n\"fatal[27]\"),n.iter=inference.length)\n# (Convergence checks not shown in the document)\nplot(results.time.B)\ngelman.plot(results.time.B)\ngelman.diag(results.time.B)\nautocorr.plot(results.time.B[[1]][,\"beta0\"],main=\"Poisson rate b0\")\nautocorr.plot(results.time.B[[1]][,\"beta1\"],main=\"Poisson rate b1\")\neffectiveSize(results.time.B[[1]][,\"beta0\"])\neffectiveSize(results.time.B[[1]][,\"beta1\"])\nsummary(results.time.B)\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 2.  **Binary data: Low Birth Weights.**\n\n**These birth weight data for 189 infants born in Massachusetts,\nUSA, are from Hosmer and Lemeshow (2000; Applied Logistic Regression). The dataset `lowbwt.RData` is available on\n    Learn but it will be automatically uploaded by the code below. The primary response\nvariable, `LowBwt`, is an indicator for whether or not infant's birth weight was less than 2500g\n(LowBwt = 1 if `Bwt`<2500g, 0 otherwise). There are several potential covariates, including:**\n\n- **`Mother.age`**\n- **`Mother.wt`**\n- **`Race`(1,2,3 for white, black, and other)**\n- **`Smoke`(1 for yes, 0 for no)**"},{"metadata":{"trusted":true},"cell_type":"code","source":"system(\"wget --no-check-certificate -r 'https://drive.google.com/uc?export=download&id=1ptk7a1NN2pUqeWV-afUEKetMIwVB76xU' -O /kaggle/working/lowbwt.RData\")\n# You need to enable the Internet in Settings in Kaggle (right hand side menu) before running this\nload(\"/kaggle/working/lowbwt.RData\")\n#The loaded data is contained in the bwt dataframe\nhead(bwt)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**(i) Perform some exploratory data analysis and comment your results.**"},{"metadata":{"trusted":true},"cell_type":"code","source":"par(mfrow=c(2,3))\nscatter.smooth(bwt$Bwt ~ bwt$Mother.age,main=\"Bwt vs Age\")\nscatter.smooth(bwt$Bwt ~ bwt$Mother.wt,main=\"Bwt vs Mother\ns Wt\")\nboxplot(split(bwt$Mother.age,bwt$LowBwt),main=\"Age of LowBwt vs Age of Normal\",\nnames=c(\"LowBwt\",\"Normal\"))\nboxplot(split(bwt$Mother.wt,bwt$LowBwt),main=\"Mother\ns Wt for LowBwt vs Normal\",\nnames=c(\"LowBwt\",\"Normal\"))\nboxplot(split(bwt$Bwt,bwt$Race),main=\"Bwt vs Race\",\nnames=c(\"White\",\"Black\",\"Other\"))\nboxplot(split(bwt$Bwt,bwt$Smoke),main=\"Bwt vs Smoking Status\",\nnames=c(\"Not Smoke\",\"Smoke\"))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**(ii) Use `glm` to fit the following 3 binomial models for the logistic transformation of the probability\nof a low birthweight, $p$. The continuous covariates are being standardized, not just centred.**\n\n- **(A) $\\log(p/(1-p))= \\beta_0 + \\beta_1 \\dfrac{\\text{Mother.age}-\\overline{\\text{Mother.age}}}{sd_\\text{Mother.age}}$**\n- **(B) $\\log(p/(1-p))= \\beta_0 + \\beta_1 \\dfrac{\\text{Mother.wt}-\\overline{\\text{Mother.wt}}}{sd_\\text{Mother.wt}}$**\n- **(C) $\\log(p/(1-p))= \\beta_0 + \\beta_1 I_\\text{Smoke}$**\n\n**Here's example R code for the model (A):**"},{"metadata":{"trusted":true},"cell_type":"code","source":"bwt$age.std <- scale(bwt$Mother.age)[,1]\nm.age <- glm(LowBwt ~ age.std,family=binomial(link=\"logit\"),data=bwt)\ncoef(m.age)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Note: when the data are Bernoulli (n=1), then a vector of 1's and 0's can be used as the\nresponse variable in the `glm` function. Interpret the slope coeffcients for the 3 models.\nE.g., as mother's age increases of one standard deviation what\nhappens, on average, to the odds of low birthweight infant?"},{"metadata":{"trusted":true},"cell_type":"code","source":"bwt$age.std <- scale(bwt$Mother.age)[,1]\nbwt$wt.std <- scale(bwt$Mother.wt)[,1]\nm.age <- glm(LowBwt ~ age.std,family=binomial(link=\"logit\"),data=bwt)\ncoef(m.age)\n## (Intercept) age.std\n## -0.804115 -0.271043\nm.wt <- glm(LowBwt ~ wt.std,family=binomial(link=\"logit\"),data=bwt)\ncoef(m.wt)\n## (Intercept) wt.std\n## -0.8266562 -0.4298929\nm.smoke <- glm(LowBwt ~ Smoke,family=binomial(link=\"logit\"),data=bwt)\ncoef(m.smoke)\n## (Intercept) Smoke\n## -1.0870515 0.7040592","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**(iii) Carry out a Bayesian analysis for the Mother's age model (A) using JAGS.**\n\n**Use Gaussian prior distributions for $\\beta_1$ with mean 0. Place prior precisions small enough so that the\nchange of probability in the inverse-logit curve can happen in an interval ($x_u-x_l$) of 0.5 units of the\nstandardized covariate and the centre of the probability change ($x_m$) can happen anywhere between\n-3 and 3 (Hint: Look at the relations between ($x_u-x_l$), $\\beta_1$, $x_m$ and $\\beta_0$ in slide 56 of Lecture 4).**\n\n**Check sensitivity to priors by trying smaller or larger precisions for $\\beta_1$ and $\\beta_0$.\nUse at least 3 sets of initial values of $\\beta_0$ and $\\beta_1$ so that the BGR statistic can be calculated.\nCompare the results to the classical point estimates from (ii).**"},{"metadata":{"trusted":true},"cell_type":"code","source":"lowbwt.model <- \"model {\n# data that will be read in are n, LowBwt, x=bwt$Mother.age\ntau0 <- ###\ntau1 <- ###\nbeta0 ~ dnorm(0,tau0)\nbeta1 ~ dnorm(0,tau1)\n#Likelihood\nfor(i in 1:n){\nlogit(p[i]) <- beta0+(beta1*x[i]-mean(x[]))/sd(x[])\n# LowBwt[i] ~ dbern(p[i]) # alternative distribution when Bernoulli\nLowBwt[i] ~ dbin(p[i],1)}\n}\"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Create data list\nlowbwt.data <- list(n=nrow(bwt),LowBwt=bwt$LowBwt,x=bwt$Mother.age)\n# Create initial values for JAGS\nnum.chains <- 3\nlowbwt.inits <- list(list(beta0=-1,beta1=0),\nlist(beta0=1,beta1=-1),\nlist(beta0=2,beta1=1))\n# Create model block for JAGS\nlowbwt.model <- \"model {\n# data that will be read in are n, LowBwt, x\n# prior\ntau0 <- 0.00028 # 1/60^2\ntau1 <- 0.0025 # 1/20^2\nbeta0 ~ dnorm(0,tau0)\nbeta1 ~ dnorm(0,tau1)\n#Likelihood\nfor(i in 1:n) {\nlogit(p[i]) <- beta0+beta1*(x[i]-mean(x[]))/sd(x[])\n# LowBwt[i] ~ dbern(p[i]) #an alternative when #trials=1\nLowBwt[i] ~ dbin(p[i],1)}\n}\"\n#Run JAGS to the completion of the \"adaption\" stage\nburnin <- 5000\ninference.length <- 10000\nresults.A <- jags.model(file=textConnection(lowbwt.model),\ndata=lowbwt.data, inits=lowbwt.inits,\nn.chains=num.chains, quiet = TRUE)\n#\nupdate(results.A, n.iter=burnin)\n#\nresults.B <- coda.samples(results.A,\nvariable.names=c(\"beta0\",\"beta1\"),n.iter=inference.length)\n# Convergence checks \nplot(results.B)\ngelman.plot(results.B)\ngelman.diag(results.B)\nautocorr.plot(results.B[[1]][,\"beta0\"],main=\"Intercept\")\nautocorr.plot(results.B[[1]][,\"beta1\"],main=\"Slope\")\neffn.b0 <- effectiveSize(results.B[[1]][,\"beta0\"])\neffn.b1 <- effectiveSize(results.B[[1]][,\"beta1\"])\ncat(\"Given a chain of length\", inference.length,\"Effective n=\",\nround(effn.b0),round(effn.b1),\"nn\")\n#\nsummary(results.B)\n# compare to glm results\nprint(coef(m.age))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**(iv) This exercise is to give you experience creating indicator variables in JAGS when the covariate\nis categorical. Model the logit transform of $p$, the probability of low birthweight, as a function\nof race. Create 2 indicator variables, one for White and one for Black, thus when both\nindicators equal 0, the race is Other. You can use the following code lines to include this\nindicator variables into you data frame.**"},{"metadata":{"trusted":true},"cell_type":"code","source":"n <- nrow(bwt)\nbwt$White.Ind <- bwt$Black.Ind <- numeric(n)\nbwt$White.Ind[bwt$Race==1] <- 1\nbwt$Black.Ind[bwt$Race==2] <- 1","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**(iv a) Run the JAGS model with random initial values. Choose non-informative prior distributions\n(in case of doubt, perform a sensitivity to priors analysis). Conduct the usual convergence diagnostics.**\n\n**Use the posterior samples to compute the expected posterior odds ratios for each race; e.g.,**\n$$\\dfrac{Pr(\\text{LowBwt|Other})}{1-Pr(\\text{LowBwt|Other})}=\\exp{(\\beta_0)}$$\n\n$$\\dfrac{Pr(\\text{LowBwt|White})}{1-Pr(\\text{LowBwt|White})}=\\exp{(\\beta_0+\\beta_\\text{White})}$$\n\n**Calculate also $90\\%$ credible intervals for these odds ratios. You can use the function\n`do.call(rbind.data.frame, results.race.B)`, where `results.race.B` is the object containing the MCMC chains, to combine the simulations of the $\\beta$s and directly\nmanipulate them or modify your JAGS model to include these computed variables and\nre-run the code.**\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"lowbwt.race.data <- list(n=dim(bwt)[1], LowBwt=bwt$LowBwt,\nWhite.Ind=bwt$White.Ind, Black.Ind=bwt$Black.Ind)\n# Create initial values for JAGS\nnum.chains <- 3\nlowbwt.race.inits <- function(){list(beta0= rnorm(0,2),\nb.White= rnorm(0,2),\nb.Black= rnorm(0,2)) }\n# Create model block for JAGS\nlowbwt.race.model <- \"model{ \n# prior\ntau <- 0.001\nbeta0 ~ dnorm(0,tau)\nb.White ~ dnorm(0,tau)\nb.Black ~ dnorm(0,tau)\n#Likelihood\nfor(i in 1:n) {\nlogit(p[i]) <- beta0 + b.White*White.Ind[i] + b.Black*Black.Ind[i]\nLowBwt[i] ~ dbin(p[i],1)\n}\n}\"\n# Run JAGS to the completion of the \"adaption\" stage\nburnin <- 5000\ninference.length <- 10000\nresults.race.A <- jags.model(file=textConnection(lowbwt.race.model),\ndata=lowbwt.race.data, inits=lowbwt.race.inits,\nn.chains=num.chains, quiet = TRUE)\nupdate(results.race.A, n.iter=burnin)\nresults.race.B <- coda.samples(results.race.A,\nvariable.names=c(\"beta0\",\"b.White\",\"b.Black\"),n.iter=inference.length)\n# (Convergence checks not shown in the document)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fit.const<-do.call(rbind.data.frame, results.race.B)\nORothers <- exp(fit.const$beta0)\nORblack <- exp(fit.const$beta0 + fit.const$b.Black)\nORwhite <- exp(fit.const$beta0 + fit.const$b.White)\nmean(ORothers)\n## [1] 0.608\nquantile(ORothers, c(0.05,0.95))\n## 5% 95%\n## 0.390 0.880\nmean(ORblack)\n## [1] 0.785\nquantile(ORblack, c(0.05,0.95))\n## 5% 95%\n## 0.367 1.397\nmean(ORwhite)\n## [1] 0.320\nquantile(ORwhite, c(0.05,0.95))\n## 5% 95%\n## 0.207 0.457","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**(iv b) Knowing the actual probabilities of an outcome is also of interest. Compute the following quantities and the relative $95\\%$ CI by modifying your JAGS code or directly manipulating the MCMC\nsimulations:**\n$$Pr(\\text{LowBwt|Other})=inv.logit(\\beta_0)$$\n\n$$Pr(\\text{LowBwt|Black})=inv.logit(\\beta_0+ \\beta_\\text{Black})$$\n\n$$Pr(\\text{LowBwt|White})=inv.logit(\\beta_0+ \\beta_\\text{White})$$\n\n**Note: In JAGS `ilogit` is the inverse of the logit function returning a value in (0,1). In\n`R` you should define the function yourself.**"},{"metadata":{"trusted":true},"cell_type":"code","source":"invlogit <- function(x){1/(1+exp(-x))}\nPothers <- invlogit(fit.const$beta0)\nPblack <- invlogit(fit.const$beta0 + fit.const$b.Black)\nPwhite <- invlogit(fit.const$beta0 + fit.const$b.White)\nmean(Pothers)\n## [1] 0.373\nquantile(Pothers, c(0.05,0.95))\n## 5% 95%\n## 0.281 0.458\nmean(Pblack)\n## [1] 0.423\nquantile(Pblack, c(0.05,0.95))\n## 5% 95%\n## 0.268 0.583\nmean(Pwhite)\n## [1] 0.240\nquantile(Pwhite, c(0.05,0.95))\n## 5% 95%\n## 0.171 0.314","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**(v) Implement the Bayesian analysis of the mother's age model (A) in INLA using the same priors as in (iii). Compare the results with what you have obtained in (iii).\nDiscuss how would you be able to also include the covariate `Mother.age` and the categorical covariates `Race` and `Smoke` in an alternative model. Compare how well these two models fit the data using marginal likelihood, DIC, and NLSCPO criteria (the first one only using the mother's weight, while the second one using all 4 covariates).\n<br>\nHint: in INLA, binary data with logistic link function can be handled by the call\n<br>\ninla(formula,family=\"binomial\", control.family=list(link=\"logit\"),data=data,...)\n<br>\nIf there are multiple trials in each row (which is not the case here), the Ntrials argument has to be used to indicate the number of trials (see the code for Lecture 4 for an example on the Beetles dataset).**"},{"metadata":{},"cell_type":"markdown","source":"The code below loads INLA."},{"metadata":{"trusted":true},"cell_type":"code","source":"#This code unzips an installation of R-INLA from an online source, and loads INLA\n#IMPORTANT: Go to the Kaggle Settings (right hand side) and enable the Internet option before running this.\nsystem(\"wget --no-check-certificate -r 'https://uoe-my.sharepoint.com/:u:/g/personal/dpaulin_ed_ac_uk/EUNBvDg_EJVFqSZJA3Xz7LsB5cVgqYk0HWWnOp74_Dr28A?download=1' -O /kaggle/working/kaggle_INLA.zip\")\nsystem(\"unzip /kaggle/working/kaggle_INLA.zip\")\nsystem(\"rm /kaggle/working/kaggle_INLA.zip\")\nlibrary(INLA,lib.loc=\"/kaggle/working\")\n#If INLA has been successfully loaded, you should see the following:\n#This is INLA_20.03.17 built 2021-01-02 20:27:47 UTC.\n#See www.r-inla.org/contact-us for how to get help.\n#To enable PARDISO sparse library; see inla.pardiso()\n\n#The following code does the full installation. You can try it if the previous code fails, but this takes longer.\n#install.packages(\"INLA\",repos=c(getOption(\"repos\"),INLA=\"https://inla.r-inla-download.org/R/stable\"), dep=TRUE,lib=\"/kaggle/working\")\n#library(INLA,lib.loc=\"/kaggle/working\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#### Bayesian analysis for the Low Birth Weights data with INLA####\n\n#This list encodes the means and precisions of the Gaussian prior for the regression coefficients beta\n#This is the same prior that we used for JAGS\nprior.beta <- list(mean.intercept = 0, prec.intercept = 0.00028,\n                    mean = 0, prec = 0.0025)\n\n#We create standardized covariates for mother's age and weight\nbwt$age.std <- scale(bwt$Mother.age)[,1]\nbwt$wt.std <- scale(bwt$Mother.wt)[,1]\n\n#We fit the model (A) based on the mother's age with INLA\n\nm.age.I <- inla(LowBwt ~ age.std,family=\"binomial\", control.family=list(link=\"logit\"), control.fixed=prior.beta, data = bwt,control.compute=list(cpo=TRUE,dic=TRUE))\nsummary(m.age.I)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The results are very similar to what we have obtained in (iii).\nThe regression coefficient for mother's age (standardized) has posterior mean of -0.273, meaning that older mothers are less likely to give birth to children with low birth weights.\nThis is somewhat contrary to the common knowledge that the chance of birth defects increases with the mother's age, which suggests that other age related factors might be at play for this particular birth defect (such as economic deprivation)."},{"metadata":{"trusted":true},"cell_type":"code","source":"#We fit another model using the mother's age, weight, race, and whether they smoke\nn <- nrow(bwt)\nbwt$Race.fct=vector(mode=\"character\",length=n)\nbwt$Race.fct[bwt$Race==1] <- 'White'\nbwt$Race.fct[bwt$Race==2] <- 'Black'\nbwt$Race.fct[bwt$Race==3] <- 'Other'\nbwt$Race.fct=as.factor(bwt$Race.fct)\n\nprior.beta <- list(mean.intercept = 0, prec.intercept = 0.00028,\n                    mean = 0, prec = 0.0025)\n\n\nm.4.cov.I <- inla(LowBwt ~ age.std+wt.std+Race.fct+Smoke,family=\"binomial\", control.family=list(link=\"logit\"), control.fixed=prior.beta, data = bwt,control.compute=list(cpo=TRUE,dic=TRUE))\nsummary(m.4.cov.I)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"In this case, we can see that the effect of the age is much less significant (posterior mean of regression coefficient becomes -0.121 compared to 0.273). The mother's weight is an important factor, with increased mother's weight reduces the chances of low birth weight. Smoking increases the chance of low birth weight significantly, while having white race reduces the chances of low birth weight dramatically when compared to black or other races."},{"metadata":{},"cell_type":"markdown","source":"Finally, we print out the model comparison criteria below."},{"metadata":{"trusted":true},"cell_type":"code","source":"cat(\"Marginal log-likelihood of model 1:\",m.age.I$mlik[1],\"\\n\")\ncat(\"Marginal log-likelihood of model 2:\",m.4.cov.I$mlik[1],\"\\n\")\n\ncat(\"DIC of model 1:\",m.age.I$dic$dic,\"\\n\")\ncat(\"DIC of model 2:\",m.4.cov.I$dic$dic,\"\\n\")\n\ncat(\"NSLCPO of model 1:\",-sum(log(m.age.I$cpo$cpo)),\"\\n\")\ncat(\"NSLCPO of model 2:\",-sum(log(m.4.cov.I$cpo$cpo)),\"\\n\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"So according to DIC and NLSCPO, the second model including 4 covariates gives a better fit on the data. On the contrary, according to the marginal likelihood, the first model is better than the second.\nHowever, marginal likelihood can be more sensitive to the choice of the prior than the other two criteria, and less sensitive to the fit on the data, so the other two criteria should be given higher importance when comparing these models,\nespecially when our goal is prediction (i.e. NSLCPO is a cross validation type criteria, and DIC is also closer to cross validation than the marginal likelihood)."}],"metadata":{"kernelspec":{"name":"ir","display_name":"R","language":"R"},"language_info":{"name":"R","codemirror_mode":"r","pygments_lexer":"r","mimetype":"text/x-r-source","file_extension":".r","version":"3.6.3"}},"nbformat":4,"nbformat_minor":4}