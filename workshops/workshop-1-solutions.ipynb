{"cells":[{"metadata":{},"cell_type":"markdown","source":"**University of Edinburgh**\\\n**School of Mathematics**\\\n**Bayesian Data Analysis, 2020/2021, Semester 2**\\\n**Daniel Paulin & Nicolò Margaritella**\n\n**Solutions for Workshop 1: Introduction to Bayesian inference in `R`**\n"},{"metadata":{},"cell_type":"markdown","source":"1.  **Analysis of binomial data: drug**. Consider the example from\nlecture 1 where a new drug is being considered for relief of chronic\npain, with the success rate $\\theta$ being the proportion of\npatients experiencing pain relief. In the past, drugs of this type\nhave shown variable pain relief rates, with a mean of $40\\%$ and a\nstandard deviation of $10\\%$. We have seen that these could be\ntranslated into a $\\text{Beta}(9.2,13.8)$ distribution. This drug\nhad 15 successes out of 20 patients.\n\n**(i) Calculate the posterior distribution of the success rate $\\theta$.**\n\n    "},{"metadata":{},"cell_type":"markdown","source":"The posterior distribution of the success rate is\n        $$\\begin{aligned}\n        p(\\theta\\mid y)&\\propto f(y\\mid\\theta)\\pi(\\theta)\\\\\n        &=\\binom{n}{y}\\theta^{y}(1-\\theta)^{n-y}\\frac{1}{B(a,b)}\\theta^{a-1}(1-\\theta)^{b-1}\\\\\n        &\\propto \\theta^{a+y-1}(1-\\theta)^{b+n-y-1},\\end{aligned}$$\n        which we recognise as the kernel of a beta distribution with\n        parameters $a+y$ and $b+n-y$. Therefore,\n        $$\\theta\\mid y\\sim\\text{Beta}(a+y,b+n-y).$$ Taking $a=9.2$,\n        $b=13.8$, $n=20$, and $y=15$, results in a\n        $\\text{Beta}(24.2,18.8)$ distribution.\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"alpha.prior <- 9.2\nbeta.prior  <- 13.8\nn           <- 20\nnum.succ    <- 15\np.hat       <- num.succ/n\n\n# (a) The posterior dist'n is alpha' = alpha.prior + x\n#                         and beta'  = beta.prior + n-x\nalpha.post  <- alpha.prior+num.succ\nbeta.post   <- beta.prior +n-num.succ\ncat(\"Posterior alpha=\",alpha.post,\"and beta=\",beta.post,\"\\n\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**(ii) What is the posterior mean and $95\\%$ highest posterior density (HPD) interval for the response rate?\\\n   *Hint*: For computing the HPD interval you can use, for instance, the function `hpd` from the `R` package `TeachingDemos`.**\n"},{"metadata":{},"cell_type":"markdown","source":"The posterior mean is $24.2/(24.2+18.8)=0.563$. Using the\n         function `hpd` from the package `TeachingDemos` (see `R`\n         code below), we obtain the HPD interval $(0.416,0.708)$."},{"metadata":{"trusted":true},"cell_type":"code","source":"# Q ii. posterior mean=a'/(a'+b') \ncat(\"Posterior mean=\",alpha.post/(alpha.post+beta.post),\"\\n\")\n\nrequire(TeachingDemos)\ncat(\"The 95% HPD for probability of success is\",\n    hpd(qbeta, shape1=alpha.post, shape2=beta.post),\"\\n\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**(iii) Compute a symmetric $95\\%$ credible interval. Compare this to the $95\\%$ HPD interval.**\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"cat(\"0.025 and 0.975=\",qbeta(p=c(0.025,0.975),shape1=alpha.post,\n                             shape2=beta.post))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"By computing the $2.5\\%$ and $97.5\\%$ percentiles of the\nposterior distribution, we obtain the symmetric credible\ninterval $(0.414,0.706)$. The two intervals (HPD and credible)\nare different but close because in this case the posterior\ndistribution is unimodal and almost symmetric around the mean (see plot below)."},{"metadata":{"trusted":true},"cell_type":"code","source":"x <- seq(0.1,0.99,by=0.01)\nplot(x,dbeta(x,shape1=alpha.post,shape2=beta.post),type=\"l\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**(iv) What is the probability that the true success rate is greater than $0.6$?**\n\n"},{"metadata":{},"cell_type":"markdown","source":"The probability that the true success rate is greater than\n$0.6$ is $0.316$."},{"metadata":{"trusted":true},"cell_type":"code","source":"#Q iv. Pr(p >0.6)\ncat(\"Pr(p>0.6)=\",1-pbeta(0.6,shape1=alpha.post,shape2=beta.post),\"\\n\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**(v) How is this value affected if a uniform prior is adopted? And how is it affected in the case that Jeffreys' prior is adopted?**\n"},{"metadata":{},"cell_type":"markdown","source":"Under a uniform prior, i.e., with a $\\text{Beta}(1,1)$ prior\ndistribution, the above probability changes to $0.904$. With a\nJeffreys' prior, it is $0.918$."},{"metadata":{"trusted":true},"cell_type":"code","source":"#Q v. with Uniform prior\nalpha.post2 <- 1+num.succ\nbeta.post2  <- 1+n-num.succ\ncat(\"Pr(p>0.6)=\",1-pbeta(0.6,shape1=alpha.post2,shape2=beta.post2),\"\\n\")\n\n#Q v. with Jeffreys prior\nalpha.post2 <- 0.5+num.succ\nbeta.post2  <- 0.5+n-num.succ\ncat(\"Pr(p>0.6)=\",1-pbeta(0.6,shape1=alpha.post2,shape2=beta.post2),\"\\n\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**(vi) Using the original $\\text{Beta}(9.2,13.8)$ prior, suppose $40$ more patients were entered into the study.\nWhat is the chance that at least $25$ of them experience pain relief? *Hint*:\nYou might want to use the `beta` and `gamma` functions implemented in `R`.**"},{"metadata":{},"cell_type":"markdown","source":"Let $z$ denotes the number of positive responses in further\n $m=40$ patients. We must first calculate the posterior\n predictive distribution\n\n $$\\begin{aligned}\n f(z\\mid y)&=\\int_{\\Theta}f(z\\mid\\theta)p(\\theta\\mid y)\\text{d}\\theta\\\\\n &=\\int_{0}^{1}\\binom{m}{z}\\theta^{z}(1-\\theta)^{m-z}\\frac{1}{B(a+y,b+n-y)}\\theta^{a+y-1}(1-\\theta)^{b+n-y-1}\\text{d}\\theta\\\\\n &=\\binom{m}{z} \\frac{1}{B(a+y,b+n-y)}\\int_{0}^{1}\\theta^{a+y+z-1}(1-\\theta)^{b+n-y+m-z-1}\\text{d}\\theta\\\\\n &=\\binom{m}{z} \\frac{B(a+y+z,b+n-y+m-z)}{B(a+y,b+n-y)}\\int_{0}^{1}\\frac{1}{B(a+y+z,b+n-y+m-z)}\\theta^{a+y+z-1}(1-\\theta)^{b+n-y+m-z-1}\\text{d}\\theta\\\\\n &=\\binom{m}{z} \\frac{B(a+y+z,b+n-y+m-z)}{B(a+y,b+n-y)}\\end{aligned}$$\n\n This is the Beta-Binomial Distribution. It is now\n straightforward to find that $\\Pr(z\\geq 25)=0.329$ (see `R`\n script, using the CDF function pbbinom from the package\n extraDistr).\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"#Q vi. Posterior marginal for event (posterior predictive dist'n):\n# Pr(x>=25|n=40); need the beta-binomial distribution\n# This is installed by default in Kaggle.\n#In case it is not installed for you, it can be loaded using the package extraDistr\n#install.packages(\"extraDistr\")\nlibrary(extraDistr)\ncat(\"Posterior Pr(X>=25|n=40) based on Beta-Binom(alpha.post, beta.post, n) \\n\")\n\ncat(pbbinom(q = 24, size = 40, alpha = alpha.post, beta = beta.post, lower.tail = FALSE),\"\\n\")\n#lower.tail = FALSE means that we compute P(X>x) and not P(X<=x) (which is the default lower.tail = TRUE case)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**(vii) We might ask whether the observed data is 'compatible' with\nthe expressed prior distribution. One method is to calculate\nthe predictive probability of observing such an extreme number\nof successes under this prior: this is a standard $p$-value\nbut where the null hypothesis is a distribution. Use the\npredictive distribution for 20 future patients to find the\nprobability of getting at least $15$ successes (i.e., at least\n$15$ patients experiencing pain relief). Do you think this\nsuggests the data are incompatible with the prior?**\n\n"},{"metadata":{},"cell_type":"markdown","source":"We start by calculating the prior predictive distribution\n  $$\\begin{aligned}\n  f(y)&=\\int_{\\Theta}f(y\\mid\\theta)p(\\theta)\\text{d}\\theta\\\\\n  &=\\int_{0}^{1}\\binom{n}{y}\\theta^{y}(1-\\theta)^{n-y}\\frac{1}{B(a,b)}\\theta^{a-1}(1-\\theta)^{b-1}\\text{d}\\theta\\\\\n  &=\\binom{n}{y}\\frac{1}{B(a,b)}\\int_{0}^{1}\\theta^{a+y-1}(1-\\theta)^{b+n-y-1}\\text{d}\\theta\\\\\n  &=\\binom{n}{y}\\frac{B(a+y,b+n-y)}{B(a,b)}\\end{aligned}$$ The\n  prior predictive probability of observing at least 15 positive\n  responses can then be computed from the last expression and it\n  is 0.01526 (see `R` script for further details). This suggests\n  some evidence that the data and the prior are incompatible."},{"metadata":{"trusted":true},"cell_type":"code","source":"#Q vii. Bayesian P-value based on the Prior\n#    Probability of at least 15 successes in n=20 trials. \ncat(\"Prior Pr(X>=15|n=20) based on Beta-Binom(alpha, beta, n) \\n\")\ncat(pbbinom(q = 14, size = 20, alpha = alpha.prior, beta = beta.prior, lower.tail = FALSE),\"\\n\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**(viii) Check for prior/data conflict by making the\nprior/likelihood/posterior plot.**"},{"metadata":{},"cell_type":"markdown","source":"The prior/likelihood/posterior plot is shown in the figure\nbelow. There is not much overlap between the support of the prior\nand the likelihood and the prior has considerable effect on\nthe posterior. Re-doing the same plot but now with Jeffreys'\nprior we can appreciate that now the prior has basically no\neffect on the posterior, with most information coming from\nthe likelihood.\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"#Q viii.  pictures of densities and obs'n, and likelihood- 10-15% overlap?\ntheta <- seq(0.0,0.99,by=0.01)\nbinomial.likelihood.norm.constant <- \n  integrate(f=function(theta) { dbinom(x=num.succ,size=n,prob=theta)},\n            lower=0,upper=1)$value\ncat(\"Normalizing constant:\", binomial.likelihood.norm.constant,\"\\n\")\n\nprior.dens <- dbeta(x=theta,shape1=alpha.prior,shape2=beta.prior)\npost.dens  <- dbeta(x=theta,shape1=alpha.post, shape2=beta.post)\nlikelihood <- dbinom(x=num.succ,size=n,prob=theta)/\n  binomial.likelihood.norm.constant\nmy.ylim <- range(c(prior.dens,post.dens,likelihood))\nplot(theta,prior.dens,type='l',col='blue',ylim=my.ylim,xlab='Pr(Success)',\n     ylab='',main='Prior, Likelihood, Posterior for Pr(Success)')\nlines(theta,likelihood,col='green',lty=2)\nlines(theta,post.dens,col='red',lty=3)\nlegend('topleft',legend=c('Prior','Likelihood','Posterior'),\n       col=c('blue','green','red'),lty=1:3,bty=\"n\" )","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"2. **Analysis of drug data with mixture priors**. In the previous \nexample, suppose that most drugs $(95\\%)$ are assumed to come from\nthe stated $\\text{Beta}(9.2,13.8)$ prior, but there is a small\nchance that the drug might be a 'winner'. 'Winners' are assumed to\nhave a prior distribution with mean $0.8$ and standard deviation\n$0.1$.\n\n**(i) What Beta distribution might represent the 'winners' prior?\nRemember that a $\\text{Beta}(a,b)$ distribution has mean\n$\\mu=a/(a+b)$ and variance $\\sigma^2=ab/\\{(a+b)^2(a+b+1)\\}$.**\n"},{"metadata":{},"cell_type":"markdown","source":"Note that by rearrangement, we have $a+b=a/\\mu$, and so $b=a(1-\\mu)/\\mu$. By substituting this into the equation for $\\sigma^2$, we obtain that $\\sigma^2=a^2((1-\\mu)/\\mu)/\\{(a/\\mu)^2+(a/\\mu)^3\\}$.\nSolving this $a$ and $b$ gives a $\\text{Beta}(12,3)$ prior."},{"metadata":{"trusted":true},"cell_type":"code","source":"#Q i. parameters of Beta such that mu=0.8 and sigma=0.1\n# solving for Beta shape parameters given mu and sigma\n# showing this requires a few lines of calculations on a paper \nbeta.param.calc <- function(mu,sigma) {\n  alpha <- (mu^2*(1-mu)-mu*sigma^2)/sigma^2\n  beta  <- alpha/mu-alpha\n  return(list(alpha=alpha,beta=beta))\n}\n\nwinner.beta.par <- beta.param.calc(mu=0.8,sigma=0.1)\nalpha.winner <- winner.beta.par$alpha\nbeta.winner  <- winner.beta.par$beta\ncat(\"Beta dist'n for winner, shape1=\",alpha.winner,\"shape2=\",\n    beta.winner ,\"\\n\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**(ii) Plot the mixture prior.**\n"},{"metadata":{},"cell_type":"markdown","source":"The mixture prior\n $\\theta\\sim\\pi\\text{Beta}(a_1,b_1)+(1-\\pi)\\text{Beta}(a_2,b_2)$\n is plotted in the figure below."},{"metadata":{"trusted":true},"cell_type":"code","source":"#Q ii. Draw picture of the mixture prior- seems sensible\npi.1  <- 0.95\ntheta <- seq(0.00,0.99,by=0.01)\n\nprior.dens.winner <- dbeta(x=theta,shape1=alpha.winner,shape2=beta.winner)\nprior.mix.density <- pi.1*prior.dens+(1-pi.1)*prior.dens.winner\nplot(theta,prior.mix.density,xlab=\"Prob. of Success\",ylab=\"\",\n     main=\"Mixture Prior for Binomial p\",type=\"l\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**(iii) What is now the chance that the response rate is greater than\n$0.6$?\\\n*Hint*: You might start by showing that if\n$$\\theta\\sim\\pi \\text{Beta}(a_1,b_1)+(1-\\pi)\\text{Beta}(a_2,b_2),$$\nthen\n$$\\theta\\mid y\\sim \\omega_1\\text{Beta}(a_1+y,b_1+n-y)+(1-\\omega_1)\\text{Beta}(a_2+y,b_2+n-y),$$\nwhere\n$$\\omega_1= \\pi\\frac{B(a_1+y,b_1+n-y)}{B(a_1,b_1)}\\left(\\pi\\frac{B(a_1+y,b_1+n-y)}{B(a_1,b_1)}+(1-\\pi)\\frac{B(a_2+y,b_2+n-y)}{B(a_2,b_2)}\\right)^{-1}.$$\nHere $y$ denotes the number of successes.**\n\n"},{"metadata":{},"cell_type":"markdown","source":"We will start by finding the posterior distribution of\n  $\\theta$.\n\n  $$\\begin{aligned}\n  p(\\theta\\mid y)&\\propto \\binom{n}{y}\\theta^{y}(1-\\theta)^{n-y}\\left\\{\\pi\\frac{1}{B(a_1,b_1)}\\theta^{a_1-1}(1-\\theta)^{b_1-1}\n  +(1-\\pi)\\frac{1}{B(a_2,b_2)}\\theta^{a_2-1}(1-\\theta)^{b_2-1}\\right\\}\\\\\n  &\\propto \\pi\\frac{1}{B(a_1,b_1)}\\theta^{a_1+y-1}(1-\\theta)^{b_1+n-y-1}+(1-\\pi)\\frac{1}{B(a_2,b_2)}\\theta^{a_2+y-1}\n  (1- \\theta)^{b_2+n-y-1}\\\\\n  &= \\pi\\frac{B(a_1+y,b_1+n-y)}{B(a_1,b_1)}\\frac{1}{B(a_1+y,b_1+n-y)}\\theta^{a_1+y-1}(1-\\theta)^{b_1+n-y-1}\\\\\n  &~~~+(1-\\pi)\\frac{B(a_2+y,b_2+n-y)}{B(a_2,b_2)}\\frac{1}{B(a_2+y,b_2+n-y)}\\theta^{a_2+y-1}(1-\\theta)^{b_2+n-y-1}\\\\\n  &= \\pi\\frac{B(a_1+y,b_1+n-y)}{B(a_1,b_1)}\\text{Beta}(\\theta\\mid a_1+y,b_1+n-y)\\\\\n  &~~~+(1-\\pi)\\frac{B(a_2+y,b_2+n-y)}{B(a_2,b_2)}\\text{Beta}(\\theta\\mid a_2+y,b_2+n-y).\n  \\end{aligned}$$\n\n  We are almost there, but note that the 'weights'\n  $\\pi\\frac{B(a_1+y,b_1+n-y)}{B(a_1,b_1)}$ and\n  $(1-\\pi)\\frac{B(a_2+y,b_2+n-y)}{B(a_2,b_2)}$ do not sum up to\n  one. Renormalising, we finally obtain that\n  $$\\theta\\mid y\\sim\\omega_1\\text{Beta}(\\theta\\mid a_1+y,b_1+n-y)+(1-\\omega_1)\\text{Beta}(\\theta\\mid a_2+y,b_2+n-y)$$\n  with\n  $$\\omega_1=\\pi\\frac{B(a_1+y,b_1+n-y)}{B(a_1,b_1)}\\left(\\pi\\frac{B(a_1+y,b_1+n-y)}{B(a_1,b_1)}+(1-\\pi)\\frac{B(a_2+y,b_2+n-y)}{B(a_2,b_2)}\\right)^{-1}$$\n  We are now ready to compute the required probability (see `R`\n  script), which turns out to be $0.58062$."},{"metadata":{"trusted":true},"cell_type":"code","source":"#Q iii. Pr(p>0.6)? Need posterior distribution of the mixture\ny <- 15; n<- 20\nmarginal.y <- pi.1*dbbinom(x=y,size=n,alpha=alpha.prior,beta=beta.prior) +\n  (1-pi.1)*dbbinom(x=y,size=n,alpha=alpha.winner,beta=beta.winner)\n\nw1 <- pi.1*choose(n=n,k=y)*beta(alpha.prior+y,beta.prior+n-y)/\n  (beta(alpha.prior,beta.prior))\nw2 <- (1-pi.1)*choose(n=n,k=y)*beta(alpha.winner+y,beta.winner+n-y)/\n  (beta(alpha.winner,beta.winner))\nw1.scale <- w1/marginal.y\nw2.scale <- w2/marginal.y\ncat(\"wt1=\",w1.scale,\"wt2=\",w2.scale,\"sum=\",w1.scale+w2.scale,\"\\n\")\n\nalpha.winner.post=alpha.winner+y\nbeta.winner.post=beta.winner+n-y\n\ncat(\"Pr(p> 0.6)=\",w1.scale*(1-pbeta(0.6,shape1=alpha.post,shape2=beta.post)) +\n      w2.scale*(1-pbeta(0.6,shape1=alpha.winner.post,shape2=beta.winner.post)),\n    \"\\n\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**(iv) For this mixture prior, repeat the prior/data compatibility\ntest performed previously. Are the data more compatible with\nthis mixture prior?**\n\n"},{"metadata":{},"cell_type":"markdown","source":"The procedure is similar to the one in 1. (vii), the only\n difference is the computation of the prior predictive\n distribution. In this case,\n\n $$\\begin{aligned}\n f(y)&=\\int_{\\Theta}f(y\\mid\\theta)p(\\theta)\\text{d}\\theta\\\\\n &=\\int_{0}^{1}\\binom{n}{y}\\theta^{y}(1-\\theta)^{n-y}\\left\\{\\pi\\frac{1}{B(a_1,b_1)}\\theta^{a_1-1}(1-\\theta)^{b_1-1}+(1-\\pi)\\frac{1}{B(a_2,b_2)}\\theta^{a_2-1}(1-\\theta)^{b_2-1}\\right\\}\\text{d}\\theta\\\\\n &=\\pi\\binom{n}{y}\\frac{1}{B(a_1,b_1)}\\int_{0}^{1}\\theta^{a_1+y-1}(1-\\theta)^{b_1+n-y-1}\\text{d}\\theta+(1-\\pi)\\binom{n}{y}\\frac{1}{B(a_2,b_2)}\\int_{0}^{1}\\theta^{a_2+y-1}(1-\\theta)^{b_2+n-y-1}\\text{d}\\theta\\\\\n &=\\pi\\binom{n}{y}\\frac{B(a_1+y,b_1+n-y)}{B(a_1,b_1)}+(1-\\pi)\\binom{n}{y}\\frac{B(a_2+y,b_2+n-y)}{B(a_2,b_2)}\\end{aligned}$$\n\n The prior predictive probability of observing at least 15\n positive responses is now $0.0514$ (see `R` script for further\n details), which does not provide strong evidence of\n incompatibility."},{"metadata":{"trusted":true},"cell_type":"code","source":"#Q iv. Bayesian P-value calculation, based on the Mixture Prior\n# marginal dist'n is a mixture of Beta-Binom\n#    Probability of at least 15 successes in n=20 trials. \ncat(\"Prior Pr(X>=15|n=20) based on mixture of Beta-Binom \\n\")\npiece1 <- pbbinom(q=14,size=20,alpha=alpha.prior,beta=beta.prior,lower.tail = FALSE)\npiece2 <- pbbinom(q=14,size=20,alpha=alpha.winner,\n                          beta=beta.winner,lower.tail = FALSE)\ncat(pi.1*piece1+(1-pi.1)*piece2,\"\\n\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**(v) Check for prior/data conflict by making the\nprior/likelihood/posterior plot.**"},{"metadata":{},"cell_type":"markdown","source":" The prior/likelihood/posterior plot is shown in the figure below."},{"metadata":{"trusted":true},"cell_type":"code","source":"#Q v. prior/likelihood/posterior plot\npost.mix.density  <- w1.scale*dbeta(x=theta,shape1=alpha.post, shape2=beta.post) +\n  w2.scale*dbeta(x=theta,shape1=alpha.winner.post,shape2=beta.winner.post)\nbinomial.likelihood.norm.constant <- \n  integrate(f=function(theta) { dbinom(x=num.succ,size=n,prob=theta)},\n            lower=0,upper=1)$value\nlikelihood <- dbinom(x=num.succ,size=n,prob=theta)/\n  binomial.likelihood.norm.constant\nmy.ylim <- range(c(prior.mix.density,post.mix.density,likelihood))\nplot(theta,prior.mix.density,type='l',col='blue',ylim=my.ylim,xlab='Pr(Success)',\n     ylab='',main='Mixture Prior, Likelihood, Posterior for Pr(Success)')\nlines(theta,likelihood,col='green',lty=2)\nlines(theta,post.mix.density,col='red',lty=3)\nlegend('topleft',legend=c('Prior','Likelihood','Posterior'),\n       col=c('blue','green','red'),lty=1:3,bty=\"n\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"3.  **Analysis of normal data: systolic blood pressure**. Suppose we are\ninterested in the long-term systolic blood pressure (SBP), in mmHg,\nof a particular 60-year old female. We take two independent readings\nof her SBP 6 weeks apart, giving values of 127 and 133. Each\nmeasurement is assumed to be normally distributed around her\nunderlying long-term SBP $\\theta$ with standard deviation\n$\\sigma=5$.\\\nWe have additional information: a population survey revealed females\naged 60 had a mean long-term SBP of 120 with standard deviation 10.\n\n**(i) Use the information from the population survey to specify a\nnormal prior for the woman's mean SBP.**\n\n"},{"metadata":{},"cell_type":"markdown","source":"The survey information is equivalent to a normal prior for the\nmean SBP with mean 120 and variance 100."},{"metadata":{},"cell_type":"markdown","source":"**(ii) What is the posterior mean and $95\\%$ symmetric credible\ninterval for the woman's SBP? Compare this with the maximum\nlikelihood estimate and $95\\%$ confidence interval.**\n\n"},{"metadata":{},"cell_type":"markdown","source":" We have seen in class that\n $$\\theta\\mid\\mathbf{y},\\sigma^2\\sim\\text{N}\\left(\\frac{\\frac{\\mu_0}{\\sigma_0^2}+n\\frac{\\bar{\\mathbf{y}}}{\\sigma^2}}{\\frac{1}{\\sigma_0^2}+\\frac{n}{\\sigma^2}},\\frac{1}{\\frac{1}{\\sigma_0^2}+\\frac{n}{\\sigma^2}}\\right).$$\n In this case we have $\\bar{\\mathbf{y}}=130$, $n=2$,\n $\\sigma^2=25$, $\\mu_0=120$, $\\sigma_0^2=100$, leading to a\n posterior mean of $128.89$ and a 95$\\%$ credible interval of\n $(122.356,135.422)$. The MLE is 130 and a $95\\%$ confidence\n interval is\n $\\bar{y}\\pm1.96\\frac{\\sigma}{\\sqrt{n}}=(123.070,136.930)$.\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Q ii: posterior given 2 measurements of 127 and 133\nobs <- c(127,133)\n\nnormal.mu.posterior.mean <- function(mu.prior,mu.sd,sigma,ybar,n) {\n  wt.prior <- sigma^2/(sigma^2+n*mu.sd^2)\n  out <- wt.prior*mu.prior + (1-wt.prior)*ybar\n  return(out)\n}\n\nnormal.mu.posterior.sd <- function(mu.sd,sigma,n) {\n  out <- sqrt(mu.sd^2*sigma^2/(sigma^2+n*mu.sd^2))\n  return(out)\n}\n\npost.mean <- normal.mu.posterior.mean(mu.prior=120,\n                                      mu.sd=10,sigma=5,ybar=mean(obs),n=2)\npost.sd   <- normal.mu.posterior.sd(mu.sd=10,sigma=5,n=2)\ncat(\"posterior mean for mu=\",post.mean ,\"\\n\")\n# posterior mean for mu= 128.8889 \n\ncat(\"posterior sd for mu=\",post.sd ,\"\\n\")\n# posterior sd for mu= 3.33 \n\n# 95% credible interval\ncat(\"95% CI for mu=\",qnorm(p=c(0.025,0.975),mean=post.mean,sd=post.sd),\"\\n\")\n# 95% CI for mu= 122.3557 135.4221  ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**(iii) Suppose 2 additional readings were taken, both of 130. What would\nbe the $95\\%$ credible interval now?**"},{"metadata":{},"cell_type":"markdown","source":"With the 2 new observations the sample mean is unchanged,\n$\\bar{\\mathbf{y}}=130$, but now $n=4$. The 95% credible\ninterval is now $(124.658,134.165)$."},{"metadata":{"trusted":true},"cell_type":"code","source":"#Q iii: effect of 2 more readings of 130 on the posterior\nobs <- c(127,133,130,130)\npost.mean <- normal.mu.posterior.mean(mu.prior=120,\n                                      mu.sd=10,sigma=5,ybar=mean(obs),n=4)\npost.sd   <- normal.mu.posterior.sd(mu.sd=10,sigma=5,n=4)\ncat(\"posterior mean for mu=\",post.mean ,\"\\n\")\n# posterior mean for mu= 129.4118 \n\ncat(\"posterior sd for mu=\",post.sd ,\"\\n\")\n# posterior sd for mu= 2.425356  \n\n# 95% credible interval\ncat(\"95% CI for mu=\",qnorm(p=c(0.025,0.975),mean=post.mean,sd=post.sd),\"\\n\")\n# 95% CI for mu= 124.6582 134.1654  ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"4.  **Caries study: Describing caries experience in Flanders (adapted\n    from Lesaffre and Lawson, 2012, p. 37)**\\\n    The Signal-Tandmobiel study is a longitudinal oral health\n    intervention study involving a sample of 4468 children. A random\n    sample was taken by selecting primary schools at random and therein\n    all children from the first class. The children were examined in\n    1996 by 16 trained dentists (examiners) and annually thereafter for\n    6 years. Here, we look at the caries experience on primary teeth of\n    the first year of the study; hence, the data of 7-year old children\n    are evaluated here. Caries experience on primary teeth is\n    classically measured by the dmft-index. This score represents the\n    number of primary teeth that are decayed (d), missing due to\n    extraction for caries reasons (m) or filled (f) because of caries.\n    It varies from 0 (no caries experience) to 20 (all primary teeth\n    affected). We will analyse a subsample of the data formed by the\n    dmft-index of 100 children (dataset `dmft.Rdata` is available on\n    Learn). As a natural candidate for modelling the dmft-index we will\n    use a Poisson distribution, i.e.,\n    $$f(y;\\theta)=\\frac{e^{-\\theta}\\theta^y}{y!},\\quad y=0,1,2\\ldots\\quad \\theta>0.$$\n    Additionally, the following prior information is available:\n\n    -   The review paper of Vanobbergen et al. (2001) reported an\n      average dmft-index of 4.1 obtained in a study based on 109\n      seven-year-old children and conducted in Liège in 1983, while an\n      average of 1.39 was obtained around Ghent on 200 five-year-old\n      children examined in 1994.\n\n    -   It is known that oral hygiene had improved considerably in\n      Flanders in the recent years.\n\n    The authors stated, and leveraging conjugacy properties, that a\n    $\\text{Gamma}(a,b)$ prior distribution for $\\theta$ with shape $a=3$\n    and rate $b=1$ seems to adequately represent the aforementioned\n    knowledge.\n\n**(i) Conduct some exploratory data analysis. What do you think about\nthe suitability of the Poisson model for this dataset?**\n\n"},{"metadata":{},"cell_type":"markdown","source":"Hereby I will let $\\texttt{y=dmft}$. The first sensible check is\n  to look at the histogram of the data:"},{"metadata":{"trusted":true},"cell_type":"code","source":"system(\"wget --no-check-certificate -r 'https://docs.google.com/uc?export=download&id=10gM_e7ujVTvLMZhxn9gpqambkaN8ED1K' -O /kaggle/working/dmft.RData\")\n# You need to enable the Internet in Settings in Kaggle (right hand side menu) before running this\n\nload(\"/kaggle/working/dmft.RData\")\n\ny=dmft; n=length(y)\n\nhist(y,freq=F,xlab=\"dmft-index\",col=\"gray80\",ylab=\"Density\",main=\"Caries study: histogram of dmft index\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"  As can be observed the value 0 has a high frequency when\n  compared to the remaining values. A further check is to compute\n  the mean and the variance of the data (for the Poisson\n  distribution we know that the mean should be equal to the\n  variance). We obtain the values listed below, clearly showing\n  that the data is overdispersed."},{"metadata":{"trusted":true},"cell_type":"code","source":" cat(\"Mean:\",mean(y),\"\\n\")\n cat(\"Variance:\",var(y),\"\\n\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"  As the authors state in the book: \"While the Poisson\n  distribution is usually the first choice to describe the\n  distribution of counts, in medical applications it is often not\n  the best choice. For the Poisson distribution, the counts\n  represent the sum of independent events that happen with a\n  constant average. The dmft-index is the sum of binary responses\n  expressing the caries experience in each of the 20 primary\n  teeth. However, cavities in the same mouth are correlated. This\n  leads to (Poisson-) overdispersion, which means that the\n  variance is larger than the mean.\" Therefore, the Poisson\n  distribution is possibly not the best option to model this\n  dataset. Possibly, a zero inflated Poisson distribution or a\n  negative binomial distribution would be better options. For the\n  sake of simplicity, we proceed with the Poisson distribution."},{"metadata":{},"cell_type":"markdown","source":"**(ii) Using exact calculations, determine the posterior mean,\nstandard deviation and $95\\%$ credible interval for $\\theta$.**\n\n"},{"metadata":{},"cell_type":"markdown","source":"The likelihood is Poisson and $\\theta$ is assigned a\n $\\text{Gamma}(a,b)$ prior distribution. We have\n $$\\begin{aligned}\n p(\\theta\\mid\\mathbf{y})&\\propto f(\\mathbf{y};\\theta)p(\\theta)\\\\\n &=\\left\\{ \\prod_{i=1}^{n}\\frac{e^{-\\theta}\\theta^{y_i}}{y_i!}\\right\\}\\frac{b^{a}}{\\Gamma(a)}\\theta^{a-1}e^{-b\\theta}\\\\\n &\\propto e^{-n\\theta}\\theta^{\\sum_{i=1}^{n}y_i}\\theta^{a-1}e^{-b\\theta}\\\\\n &=\\theta^{a+\\sum_{i=1}^{n}y_i-1}e^{-\\theta(b+n)},\\end{aligned}$$\n i.e.,\n $\\theta\\mid\\mathbf{y}\\sim\\text{Gamma}(a+\\sum_{i=1}^{n}y_i,b+n)$.\n For this particular dataset we have $a=3$, $b=1$, $n=100$, and\n $\\sum_{i=1}^{n}y_i=217$, thus leading to\n $\\theta\\mid\\mathbf{y}\\sim\\text{Gamma}(220,101)$. For a\n $\\text{Gamma}(a_1,b_1)$ distribution, we know that its mean is\n $\\frac{a_1}{b_1}$ and its variance is $\\frac{a_1}{b_1^2}$, and\n thus\n $$E(\\theta\\mid\\mathbf{y})=\\frac{220}{101}\\approx 2.178,\\qquad \\sqrt{\\text{var}(\\theta\\mid\\mathbf{y})}=\\sqrt{\\frac{220}{101^2}}\\approx 0.147.$$\n Using the function `qgamma` in `R`, to compute the $2.5\\%$ and\n $97.5\\%$ quantiles, we obtain that a $95\\%$ credible interval\n for $\\theta$ is $(1.900,2.475)$.\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"#exact\na=3; b=1\napost=a+sum(y); bpost=b+n\nmeanpost=apost/bpost\nsdpost=sqrt(apost)/bpost\nci=qgamma(c(0.025,0.975),shape=apost,rate=bpost)\ncat(\"Mean=\",round(meanpost,3),\"SD=\",round(sdpost,3),\n    \"LB=\",round(ci[1],3),\"UB=\",round(ci[2],3),\"\\n\")\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**(iii) Repeat part (ii), but using rejection sampling. You might want\nto consider as proposal distribution an exponential\ndistribution with mean equal to the mean of the data. Comment\nabout the efficiency of the algorithm.**\n\n"},{"metadata":{},"cell_type":"markdown","source":"In order to use the rejection sampling algorithm to simulate\n  from the (unnormalised) posterior distribution, we need to\n  choose a proposal distribution. The suggestion is to use an\n  exponential distribution with mean equal to the mean of the\n  data, i.e.,\n  $$g(\\theta)=\\lambda e^{-\\lambda \\theta}, \\quad \\lambda=\\frac{1}{\\bar{\\mathbf{y}}}.$$\n  The next task is to find a constant $M$ such that\n  $$f(\\mathbf{y};\\theta)p(\\theta)\\leq M g(\\theta).$$ The optimal\n  $M$ is such that\n  $$M=\\max_{\\theta}\\left\\{\\frac{f(\\mathbf{y};\\theta)p(\\theta)}{g(\\theta)}\\right\\}.$$\n  We use the command `optimize` to find the optimal value of $M$\n  (for further details see the `R` script)."},{"metadata":{"trusted":true},"cell_type":"code","source":"#rejection sampling\n#unnormalised posterior\nunposterior=function(theta,data){\n  likelihood=prod(dpois(data,theta))    \n  prior=dgamma(theta,3,1)  \n  unpost=likelihood*prior\n  return(unpost)\n}\n\n#proposal distribution\ng=function(theta,m){\n  g=dexp(theta,rate=1/m)  \n}\n\n#auxiliar function to determine the optimal value of M; need to find maximum of unnormalised posterior/g \naux=function(theta,data){\n  likelihood=prod(dpois(x=data,lambda=theta))    \n  prior=dgamma(theta,3,1)  \n  unpost=likelihood*prior\n  g=dexp(theta,rate=1/mean(data))      \n  aux=unpost/g\n  return(aux)\n}\n\nM=optimize(aux,interval=c(0,20),data=y,maximum=TRUE)$objective\ncat(\"Optimal M=\",signif(M,4),\"\\n\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Look at M times unnormalized posterior-\nm=1000\nthetagrid=seq(0,20,len=m)\nunnormal.post.ord=numeric(m)\nfor(i in 1:m){\n  unnormal.post.ord[i]=unposterior(theta=thetagrid[i],data=y) \n}\n\nplot(thetagrid,M*g(thetagrid,m=mean(y)),type=\"l\",col=\"red\",\n     xlab=expression(theta),ylab=\"Density\")\nlines(thetagrid,unnormal.post.ord)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"From the above figure we can appreciate that there is a 'large\n  rejection area'. This is confirmed by the acceptance rate of\n  the algorithm, about $6\\%$ (see `R` script for details on the\n  implementation). The histogram of the sampled $\\theta$'s is\n  plotted below.\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"#rejection sampling algorithm\nn.samples=10000\ncount=0; attempts=0; thetapost=rep(0,n.samples)\nt1=proc.time()[\"elapsed\"]\nwhile(count<n.samples){\n  attempts=attempts+1  \n  theta.c=rexp(1,1/mean(y))\n  u=runif(1,0,1)\n  alpha=unposterior(theta=theta.c,data=y)/(M*g(theta.c,mean(y)))\n  if(u<=alpha){\n    count=count+1\n    thetapost[count]=theta.c\n  }\n}\nt2=proc.time()[\"elapsed\"]\ncat(\"Speed=\",t2-t1,\"\\n\")\ncat(\"Acceptance rate=\",round(n.samples/attempts,3),\"\\n\")\n\ncat(\"E=\",round(mean(thetapost),3), \"SD=\",round(sd(thetapost),3),\n    \"LB and UB=\",round(unlist(quantile(thetapost,c(0.025,0.975))),3),\"\\n\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"hist(thetapost,freq=F,breaks=20,xlab=expression(theta),ylab=\"Density\",main=\"\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**(iv) Repeat part (ii), but using the Metropolis-Hastings algorithm.\nPlot the ACF (autocorrelation function) and the Gelman-Rubin\ndiagnostics (see gelman.diag and gelman.plot functions in R).**"},{"metadata":{"trusted":true},"cell_type":"code","source":"library(coda)\n#The coda library contains useful diagnostics for MCMC runs\n\n#In order to avoid overflow due to numerical precision issues, \n#we work with the logarithm of the unnormalized posterior\nlogunposterior=function(theta,data){\n  loglikelihood=sum(dpois(data,theta,log=TRUE))    \n  logprior=dgamma(theta,3,1,log=TRUE)  \n  logunpost=loglikelihood+logprior\n  return(logunpost)\n}\n\nrunMH=function(data,burnin=1000,n.samples=10000){\n    theta=1; #Initializing theta\n    thetapost=rep(0,n.samples)\n\n    for (it in 1:n.samples+burnin)\n    {\n        theta.prop=theta+rnorm(1)\n        if(theta.prop<0){\n            logacc=-Inf;\n        }\n        else{\n            logacc=logunposterior(theta=theta.prop,data=y)-log(unposterior(theta=theta,data=y))\n        }\n\n        if(log(runif(1,0,1))<=logacc){\n            theta=theta.prop\n        }\n\n        if(it>burnin){\n            thetapost[it-burnin]=theta\n        }\n    }\n    return(mcmc(thetapost))\n}\n\n#Run the MCMC sampler\nt1=proc.time()[\"elapsed\"]\nsamples1=runMH(data=y)\nt2=proc.time()[\"elapsed\"]\ncat(\"Speed=\",t2-t1,\"\\n\")\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Trace plot shows good mixing\nplot(samples1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#acf also shows good mixing\nacf(samples1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"samples2=runMH(data=y)\nsamples3=runMH(data=y)\n\ncombined.chains=mcmc.list(samples1,samples2,samples3)\n\ngelman.plot(combined.chains)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"gelman.diag(combined.chains)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"As we can see, the Gelman-Rubin diagnostics show that the chains have converged.\nThe summary statistics can be seen below. This is similar to what we have obtained previously."},{"metadata":{"trusted":true},"cell_type":"code","source":"summary(combined.chains)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"name":"ir","display_name":"R","language":"R"},"language_info":{"name":"R","codemirror_mode":"r","pygments_lexer":"r","mimetype":"text/x-r-source","file_extension":".r","version":"3.6.3"}},"nbformat":4,"nbformat_minor":4}