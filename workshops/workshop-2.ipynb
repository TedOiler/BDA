{"cells":[{"metadata":{"_uuid":"051d70d956493feee0c6d64651c6a088724dca2a","_execution_state":"idle","trusted":true},"cell_type":"code","source":"# This R environment comes with many helpful analytics packages installed\n# It is defined by the kaggle/rstats Docker image: https://github.com/kaggle/docker-rstats\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":1,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**University of Edinburgh**\n\n**School of Mathematics**\n\n**Bayesian Data Analysis, 2020/2021, Semester 2**\n\n**Daniel Paulin & Nicol√≤ Margaritella**\n\n**Workshop 2: Introduction to JAGS**"},{"metadata":{},"cell_type":"markdown","source":"**Note**: Before starting this practical, you might want to spend\nsome time looking at the JAGS examples we have discussed during Lecture 2. If\nyou already did it, then go directly to question 1.\nThe code below loads JAGS."},{"metadata":{"trusted":true},"cell_type":"code","source":"#This code loads a compiled version of JAGS and rjags from a zip file on Google Drive, and loads rjags. It should only take a few seconds.\n#IMPORTANT: Go to the Kaggle Settings (right hand side) and enable the Internet option before running this.\nsystem(\"wget --no-check-certificate -r 'https://docs.google.com/uc?export=download&id=1i7BlQ21kT4ZnYUjAxa8P-eCbe-Zfiz8j' -O /kaggle/working/kaggle_JAGS.zip\")\nsystem(\"unzip /kaggle/working/kaggle_JAGS.zip\")\nsystem(\"rm /kaggle/working/kaggle_JAGS.zip\")\nsystem(\"cd /kaggle/working/JAGS-4.3.0\")\nsystem(\"make install\")\nlibrary(rjags,lib.loc=\"/kaggle/working\")\n#If it ran correctly, you should see \n#Loading required package: coda\n#Linked to JAGS 4.3.0\n#Loaded modules: basemod,bugs","execution_count":2,"outputs":[{"output_type":"stream","text":"Loading required package: coda\n\nLinked to JAGS 4.3.0\n\nLoaded modules: basemod,bugs\n\n","name":"stderr"}]},{"metadata":{},"cell_type":"markdown","source":"**1.  Analysis of binomial data: revisiting the drug example**\n\n**The aim of this question is to re-do most parts of questions 1 and 2\nfrom the first practical but now using `JAGS`. Remember the context:\na new drug is being considered for relief of chronic pain, with the\nsuccess rate $\\theta$ being the proportion of patients experiencing\npain relief. According to past information, a Beta(9.2, 13.8) prior\ndistribution was suggested. This drug had 15 successes out of 20\npatients.**\n\n**(i) Compute the posterior mean, standard deviation and a $95\\%$\ncredible interval. Compare with the exact results.**"},{"metadata":{},"cell_type":"markdown","source":"**(ii) What is the probability that the true success rate is greater\nthan 0.6. Compare with the exact result.**"},{"metadata":{},"cell_type":"markdown","source":"**(iii) Suppose 40 more patients were entered into the study. What is\nthe chance that at least 25 of them experience pain relief?\nCompare with the exact result.**"},{"metadata":{},"cell_type":"markdown","source":"**(iv) Conduct the 'prior/data compatibility check', i.e., calculate\n   the predictive probability of observing at least $15$ successes\n   under this prior. Compare with the exact result.**"},{"metadata":{},"cell_type":"markdown","source":"**(v) In practical 1 we have then considered a mixture prior, where it\nwas supposed that most drugs (95%) are assumed to come from the\nstated Beta(9.2, 13.8) prior, but there is a small chance that\nthe drug might be a 'winner'. 'Winners' were assumed to have a\n$\\text{Beta}(12,3)$ prior distribution. What is now the chance\nthat the response rate is greater than 0.6? Compare with the\nexact result.**"},{"metadata":{},"cell_type":"markdown","source":"**(vi) Under this mixture prior, what is the posterior predictive\nprobability that at least 25 out of 40 new patients experience\npain relief?**"},{"metadata":{},"cell_type":"markdown","source":"**(vii) For this mixture prior, repeat the prior/data compatibility\n    test performed previously. Are the data more compatible with\n    this mixture prior? Compare with the exact result.**"},{"metadata":{},"cell_type":"markdown","source":"2.  **Simple linear regression with robustification**\n\n**Winning Olympic Men's Long Jump Distances (adapted from\n Witmer, 2017)**\n \n**The data are the winning men's long jump distances (m) from 1900\nthrough 2008. You will fit a linear regression of the distances as a\nfunction of Olympic year: \n$$\\begin{aligned}\nJump & = & \\beta_0 + \\beta_1 \\mbox{Year} + \\epsilon\\end{aligned}$$\nthree different ways: standard frequentist approach, a Bayesian\napproach assuming normal errors, and a Bayesian approach assuming a\n$t$ distribution for errors.**\n\n**Run the following commands in `R` to begin (this will install and load the package Stat2Data and load the Long Jump dataset).**"},{"metadata":{"trusted":true},"cell_type":"code","source":"install.packages(\"Stat2Data\", lib=\"/kaggle/working\")\nlibrary(Stat2Data,lib.loc=\"/kaggle/working\")\ndata(\"LongJumpOlympics\")   #Makes the dataset available in this R session\nJump <- LongJumpOlympics$Gold\nYear <- LongJumpOlympics$Year","execution_count":3,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**1.  Carry out some exploratory data analysis:**\n\n**1.1.  Plot Jump vs Year. What does the relationship look like?**"},{"metadata":{},"cell_type":"markdown","source":"**1.2.  Fit a simple linear regression on Jump against Year using the `lm` function,\nand make a plot of the data with the fitted line overlaid using the `abline` function.**"},{"metadata":{},"cell_type":"markdown","source":"**1.3.  Based on this model, every 4 years we would expect the jump\ndistance to change by what amount?**"},{"metadata":{},"cell_type":"markdown","source":"\n**1.4.  Plot the residuals against Year (using the `residuals`\nfunction). One year stands out, which one is it?**"},{"metadata":{},"cell_type":"markdown","source":"**1.5.  For a more detailed residual analysis, type `par(mfrow=c(2,2))`, and use the `plot` function operating\non the `lm` object (you'll see 4 plots).**"},{"metadata":{},"cell_type":"markdown","source":"**1.6.  Remove the outlier from the data set and refit the model,\nthen recompute the above residual diagnostics. What do you\nobserve?**"},{"metadata":{},"cell_type":"markdown","source":"**2.  Carry out a Bayesian linear regression analysis using `rjags`.\nAs in the frequentist case assume $\\epsilon$ $\\sim$ Normal(0,\n$\\sigma^2$). Use the following priors for the three parameters:\n$$\\begin{aligned}\n\\beta_0, \\beta_1 & \\sim & \\mbox{Normal} \\left ( \\mu_0=0, \\tau_0=0.001 \\right ) \\\\\n\\tau & \\sim & \\mbox{Gamma} \\left ( a=0.1, b=0.1 \\right )\n\\end{aligned}$$**\n\n**2.1.  Write the *model* statement, which includes the likelihood\ncalculation and the prior distribution. Include a\ncalculation of $\\sigma = 1/\\sqrt{\\tau}$.**"},{"metadata":{},"cell_type":"markdown","source":"**2.2.  Create an `R` object for the data, which includes Jump,\n   Year, $n$=26 and the values of the prior hyperparameters\n   $\\mu_0$, $\\tau_0$, $a$ and $b$.**"},{"metadata":{},"cell_type":"markdown","source":"**2.3.  Create an `R` object for 3 sets of initial values; e.g.,**"},{"metadata":{"trusted":true},"cell_type":"code","source":"my.inits <- list(c(b0=0.1,b1=0.2,tau=0.1),\n           c(b0=-1,b1=3,tau=0.3),\n           c(b0=1,b1=0,tau=.8))","execution_count":4,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**2.4.  Execute `jags.model` using the above objects. Note\n`n.chains` should be set equal to 3. How many unobserved\nstochastic nodes were there? How many observed?**"},{"metadata":{},"cell_type":"markdown","source":"**2.5.  Use `update` to carry out an initial MCMC run (burn-in) with\n1,000 iterations.**"},{"metadata":{},"cell_type":"markdown","source":"**2.6.  Now make a longer MCMC run using the `coda.samples` function\nwith 10,000 iterations and have the results for $\\beta_0$,\n$\\beta_1$, and $\\sigma$ returned.**"},{"metadata":{},"cell_type":"markdown","source":"**2.7.  Plot the results from `coda.samples`. These are the trace\nplots. Do you think that the chains have converged for each\nof the 3 parameters?**"},{"metadata":{},"cell_type":"markdown","source":"**2.8.  You may have noticed from the trace plots that $\\beta_0$ and\n$\\beta_1$ are mixing slowly. That's indicative of\nsignificant autocorrelation. Use the `acf` function to see\nhow much correlation there is. For example, if the results\nfrom `coda.samples` are called `res`, for a parameter named\nbeta0:, you can write `acf(res[[1]][,\"beta0\"],lag.max=100)`.**"},{"metadata":{},"cell_type":"markdown","source":"**2.9.  Also take a look at the effective sample sizes per\nparameter, e.g.,`effectiveSize(res[[1]][,\"beta0\"])`**"},{"metadata":{},"cell_type":"markdown","source":"**2.10. In Lecture 2, the Gelman-Rubin (Brooks-Gelman-Rubin)\nstatistic was discussed. This is a quantitative measure of\napparent convergence that is based upon the degree of\noverlap of 2 or more chains after each iteration. The BGR\nstatistic roughly approximates the ratio of the variability\nbetween chains to the variability within chains (like an F\nstatistic in ANOVA). The general idea of the statistic is\nthat the the ratio of those two measures should be around 1\nat convergence, thus BGR=1 is \"good\". Use the `coda` package\nfunction called `gelman.plot` to plot the BGR statistic for\neach of the parameters against the MCMC iteration. And use\n`gelman.diag` for numerical summaries. What do you think\nabout convergence now?**"},{"metadata":{},"cell_type":"markdown","source":"**2.11. Centring the covariate, in this case Year, sometimes helps\nconvergence. Modify your Model statement slightly by\ncreating a new variable `meanY`, and then subtract that from\nthe `Year[i]` values in the for loop. Repeat the above\nsteps. How does convergence now look? Use the `summary`\nfunction on the JAGS output to examine the posterior means\nand standard deviations for $\\beta_0$, $\\beta_1$, and\n$\\sigma$. How do the posterior mean for $\\beta_1$ and\n$\\sigma$ compare to the maximum likelihood estimates\nobtained in 1.2?**"},{"metadata":{},"cell_type":"markdown","source":"**3.1.  *Robustifying the regression.* As was noted in Lecture 2, the\neffects of extreme observations or \"outliers\" on regressing\nresults can be diminished by using a $t$ distribution for the\nobservations. For simplicity, assume a $t$ distribution with 3\ndf for the distribution of errors. Revise the JAGS model code\naccordingly (continuing to work with the centred covariate) and\nre-run. Recall from Lecture 3 that the necessary change to the\ncode is to replace `dnorm` with `dt` and add an additional\nargument to `data` for the df (=3). How did the posterior mean\nof $\\beta_1$ change? Compare it to the estimate in\n1.2 when the extreme observation is removed.**"},{"metadata":{},"cell_type":"markdown","source":"**3.  Nonlinear Regression. Newton's law of cooling, from Bates and Watts (2008)**\n\n**The following data are measurements over a 41 minute period of the\ntemperature of a bore after that bore had been rubbed inside \"a\nstationary cylinder and pressed against the bottom by means of a\nscrew\". The bore was turned by a team of horses (this is an\nexperiment with friction from 1798 by a Count Rumford).**\n"},{"metadata":{"trusted":true},"cell_type":"code","source":" #minutes\n elapsed.time <- c(4,5,7,12,14,16,20,24,28,31,34,37.5,41) \n #Fahrenheit\n temperature <- c(126,125,123,120,119,118,116,115,114,113,112,111,110) ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**An underlying theoretical model based on Newton's law of cooling\nsuggests that temperature should decline over time according to the\nfollowing model. \n$$\\begin{aligned}\n\\mbox{temperature} & = & 60 +70e^{-\\theta \\;\\mbox{elapsed.time}}\n\\end{aligned}$$\nYou are to evaluate this model, i.e., make\nestimates of $\\theta$ using classical and Bayesian techniques.**\n\n**(i)  Plot temperature against time (use the `scatter.smooth` function\nto draw a nonparametric regression line through the points).**"},{"metadata":{},"cell_type":"markdown","source":"**(ii). Fit the model in (i) using a classical approach that assumes that \nobservations have model errors are iid\nNormal(0,$\\sigma^2$). \n$$\\begin{aligned}\n \\mbox{temperature} & \\sim & \\mbox{Normal} \\left ( 60 +70e^{-\\theta \\; \\mbox{elapsed.time}}, \\sigma^2 \\right )\n\\end{aligned}$$\nUse the `nls` function in R. The format of\n`nls` in this case: `nl.1 <- nls(formula= temperature ~ 60 + 70*exp(-theta*elapsed.time), start=list(theta=initial.theta))`**\n**where `initial.theta` is an initial guess as to what $\\theta$ is.**"},{"metadata":{},"cell_type":"markdown","source":"**One way to get an estimate of $\\theta$ is to \"linearize\"\nNewton's law of cooling as follows: \n$$\\begin{aligned}\n-\\ln \\left ( \\frac{(\\mbox{temperature}-60)}{70} \\right )  & = &  \\theta*\\mbox{elapsed.time}\n\\end{aligned}$$\nand then fit the resulting linear model\nwith the `lm` function.**"},{"metadata":{"trusted":true},"cell_type":"code","source":"y <- -log((temperature-60)/70)\nout <- lm(y ~ -1 + elapsed.time)  ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Use the estimated coefficient in `out` as the value of\n`initial.theta`. After fitting the model, plot the fit and\nthe observations.**"},{"metadata":{"trusted":true},"cell_type":"code","source":"plot(temperature ~ elapsed.time,xlab=\"Time\",ylab=\"\",\nmain=\"Friction Experiment Data\")\nlines(elapsed.time,fitted(nl.1),col=\"red\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**How does the fit look?**"},{"metadata":{},"cell_type":"markdown","source":"**(iii) Instead of using 60 and 70 in Newton's law of cooling as known values, refit the model\nestimating the coefficients.**"},{"metadata":{"trusted":true},"cell_type":"code","source":"nl.2 <- nls(formula=temperature ~ beta0 + beta1*exp(-theta*elapsed.time),\nstart=list(beta0=50,beta1=50, theta=initial.theta))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Compare the estimated coefficients to the assumed values and\nplot the fitted line over the top of the previous plot. Has the\nfit improved?**"},{"metadata":{},"cell_type":"markdown","source":"**(iv) Use JAGS to fit two Bayesian nonlinear regression models: one\nbased on Newton's law of cooling, as in (ii), and another where all three\ncoefficients are estimated, as in (iii). Assume that temperatures are\nnormally distributed in the likelihood model.**\n\n**In both cases use exponential distributions for the priors for\n$\\theta$ and then for $\\beta_0$ and $\\beta_1$ (to ensure that\nthe posterior distributions are positive valued). To pick the\nexponential distribution hyperparameter, say $\\alpha$, note that\nif $X \\sim$Exp$(\\alpha)$, $\\mathbb{E}[X]$ = $1/\\alpha$. Pick a\nlarge value for the hyperparameter for $\\theta$ such that the\nexpected value of $\\theta$ is less than 1. For the 2nd model (as in (iii)), select hyperparameter values for  $\\beta_0$ and $\\beta_1$ such that the expected values are 60 and\n70, respectively. Note: in JAGS, the exponential density is\nwritten `theta ~ dexp(a)` given hyperparameter $a$.**\n\n**Compare the posterior means for $\\theta$ in both models with the\nfrequentist estimates.\\\nLikewise compare the posterior means for\n$\\beta_0$ and $\\beta_1$ for the second model.**\n"},{"metadata":{},"cell_type":"markdown","source":"**4.  Multiple Linear Regression\\\nFactors Affecting Extinction Times of 62 Land Bird Species,\nadapted from Albert, 2009\\\nThe data are taken from Ramsey and Schafer (1997), who took them\nfrom Pimm et al.¬†1988, and are available in the `LearnBayes` package\nas the object `birdextinct`. Land birds on 16 small islands had been\nobserved annually during breeding surveys over a period of several\ndecades. Some 62 species went extinct at some point and the\nobjective is to examine the relationship between the years till\nextinction and three different covariates: the initial average\nnumber of nesting pairs observed (`nesting`), the physical size of\nthe birds (an indicator variable `size` with 1=small and 0=large),\nand migratory status (an indicator variable `status` with\n1=resident, 0=migratory).**\n\n**To begin, do the following in `R`.**\n\n"},{"metadata":{"trusted":true},"cell_type":"code","source":" library(LearnBayes)\n   data(birdextinct)\n   n   <- nrow(birdextinct)\n   extinct.time <- birdextinct$time\n   avg.no.nests <- birdextinct$nesting\n   size.ind  <- birdextinct$size   # 0 = large, 1= small\n   mig.ind   <- birdextinct$status # 0 = mig, 1=resident","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**1.  Exploratory data analysis and data transformation**\n\n**1.1.  Look at the histogram of `extinct.time`. It is strongly\nright skewed (there are few species with times till\nextinction that are long relative to most species).\nTherefore make the response variable the natural log of\nexinct.time:**"},{"metadata":{"trusted":true},"cell_type":"code","source":"log.extinct <- log(extinct.time)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**1.2.  Make 4 plots of $y$=`log.extinct`: histogram of $y$,\nscatterplot of $y$ against `avg.nests`, side-by-side\nboxplots of $y$ for small and large birds, and side-by-side\nboxplots of resident and migratory birds. Hint:  To make side-by-side boxplots use the `split` function; e.g.,`boxplot(split(log.extinct,size),main=‚Äôvs Size‚Äô)`**"},{"metadata":{},"cell_type":"markdown","source":"**1.3.  How would you describe the relationships between the 3\ncovariates and time till extinction?**"},{"metadata":{},"cell_type":"markdown","source":"**2. Fit a classical multiple linear regression of the `log.extinct`\non the three covariates,**"},{"metadata":{"trusted":true},"cell_type":"code","source":"extinct.mlr <- lm(log.extinct ~ avg.no.nests + size.ind + mig.ind)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**2.1 Examine the estimated coefficients. How do they compare to your\n conclusions from the EDA (Exploratory Data Analysis)?**"},{"metadata":{},"cell_type":"markdown","source":"**3. Use JAGS to fit a Bayesian multiple regression analysis.**\n\n**3.1. Centre the *avg.no.nests* covariate. Also, use 3 sets of\n initial values for the parameters.**"},{"metadata":{},"cell_type":"markdown","source":"**3.2. Plot the JAGS output to see the trace plots. (Type\n `par(ask=TRUE)` in order to see all 5 plots. Then type\n `par(ask=FALSE)` to turn the option off.)**"},{"metadata":{},"cell_type":"markdown","source":"**3.3. Use the Gelman-Rubin diagnostics to check for convergence.**"},{"metadata":{},"cell_type":"markdown","source":"**3.4. Plot the autocorrelation functions.**"},{"metadata":{},"cell_type":"markdown","source":"**3.5. Examine the effective sample sizes.**"},{"metadata":{},"cell_type":"markdown","source":"**3.6. Calculate studentised residuals, draw a QQ-plot to check\nnormality, plot posterior mean fitted values, and carry out\nposterior predictive checks for the minimum and maximum\nlog.extinct times. (See the mtcars example in the R code for Lecture 1 on the Learn site for example code to do this.)**"}],"metadata":{"kernelspec":{"name":"ir","display_name":"R","language":"R"},"language_info":{"name":"R","codemirror_mode":"r","pygments_lexer":"r","mimetype":"text/x-r-source","file_extension":".r","version":"3.6.3"}},"nbformat":4,"nbformat_minor":4}